{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EphYCemu6JlN"
      },
      "source": [
        "#Download Libraries and Frameworks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67c1ca_f5rLw"
      },
      "outputs": [],
      "source": [
        "!pip install flappy-bird-gymnasium\n",
        "!pip install gymnasium\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install cmake > /dev/null 2>&1\n",
        "!pip install --upgrade setuptools 2>&1\n",
        "!pip install ez_setup > /dev/null 2>&1\n",
        "!pip install gym[atari] > /dev/null 2>&1\n",
        "!sudo apt-get install xvfb\n",
        "!pip install swig\n",
        "!pip install gym[box2d]\n",
        "!pip install pygame"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7EUaGnz6dmU"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cSgVmI-_6Hx6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gymnasium\n",
        "import flappy_bird_gymnasium\n",
        "import pygame\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display as ipythondisplay\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers.record_video import RecordVideo\n",
        "from IPython.display import HTML\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Display video"
      ],
      "metadata": {
        "id": "HVbPbyJNAEpm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZpTPeCftIX_l"
      },
      "outputs": [],
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "\n",
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment and displaying it\n",
        "To enable video, just do \"env = wrap_env(env)\"\"\n",
        "\"\"\"\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay\n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else:\n",
        "    print(\"Could not find video\")\n",
        "\n",
        "def wrap_env(env):\n",
        "  env = RecordVideo(env, './video',  episode_trigger = lambda episode_number: True)\n",
        "  return env"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random agent"
      ],
      "metadata": {
        "id": "uPOMvELcAQ85"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ps3wfamXIxoh"
      },
      "outputs": [],
      "source": [
        "env_v = wrap_env(gymnasium.make(\"FlappyBird-v0\", render_mode=\"rgb_array\", use_lidar=False))\n",
        "observation = env_v.reset()\n",
        "\n",
        "terminated = False\n",
        "while not terminated:\n",
        "    action = env_v.action_space.sample()\n",
        "    observation, reward, terminated, truncated, info = env_v.step(action)\n",
        "\n",
        "env_v.close()\n",
        "show_video()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# define and test environment"
      ],
      "metadata": {
        "id": "ZKiFgxELAX0I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## initialize environment"
      ],
      "metadata": {
        "id": "2GpG6ihqkiW-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JzZ9tzFtJ89I"
      },
      "outputs": [],
      "source": [
        "env = gymnasium.make(\"FlappyBird-v0\", render_mode=\"rgb_array\", use_lidar=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## action space dimension"
      ],
      "metadata": {
        "id": "8qKucMD_kl2-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M__Nxe7P7G8B"
      },
      "outputs": [],
      "source": [
        "env.action_space"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## observation space or state space dimension"
      ],
      "metadata": {
        "id": "pevCBE_7ko6G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eA-Ma4i17Keg"
      },
      "outputs": [],
      "source": [
        "env.observation_space"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## reset and see a frame of environment\n",
        "- use env.step(action) for interaction with environment"
      ],
      "metadata": {
        "id": "NH6AgXX9kvwN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d5NhyD_M8WX9"
      },
      "outputs": [],
      "source": [
        "env.reset()\n",
        "print(env.step(1))\n",
        "env.render()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZBHhN7xNNKN"
      },
      "outputs": [],
      "source": [
        "a = env.step(0)\n",
        "print(a)\n",
        "env.render()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXsHhNhKLGYp"
      },
      "source": [
        "# discretizing state space"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## set the number level for discretizing"
      ],
      "metadata": {
        "id": "GYgVrfZolHcX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YFTzGlMsLFuT"
      },
      "outputs": [],
      "source": [
        "NUM_BINS = 51"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## reduce the dimension of observation by combining the dimension"
      ],
      "metadata": {
        "id": "DsCXth-2lTt_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9uRXawkcLaya"
      },
      "outputs": [],
      "source": [
        "# For creating the dimension of this problem\n",
        "# We need to decrease the dimension of problem\n",
        "# Because if the dimension is large we can'n discretising it occassionallly\n",
        "# so we use the mean of pipe\n",
        "def create_bins(num_bins):\n",
        "    horizental_pipe = np.linspace(-1.0, 1.0, num_bins)\n",
        "    mean_pipe = np.linspace(-1.0, 1.0, num_bins)\n",
        "    horizental_pipe_next = np.linspace(-1.0, 1.0, num_bins)\n",
        "    mean_pipe_next = np.linspace(-1.0, 1.0, num_bins)\n",
        "    bird_vertical_position = np.linspace(-1.0, 1.0, num_bins)\n",
        "    bird_vertical_velocity = np.linspace(-1.0, 1.0, num_bins)\n",
        "    rotation = np.linspace(-1.0, 1.0, num_bins)\n",
        "    bins = np.array([horizental_pipe,mean_pipe,horizental_pipe_next,mean_pipe_next ,bird_vertical_position,bird_vertical_velocity,rotation])\n",
        "    return bins"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZEHTF79uMsEa"
      },
      "outputs": [],
      "source": [
        "bins = create_bins(NUM_BINS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fnMK_UqvM1Ub"
      },
      "outputs": [],
      "source": [
        "# discretizing the environment\n",
        "def discretize_states(states, bins):\n",
        "    binned_states = []\n",
        "    for i, state in enumerate(states):\n",
        "        discretized_state = np.digitize(state, bins[i])\n",
        "        if discretized_state == 51:\n",
        "          discretized_state -= 1\n",
        "        binned_states.append(discretized_state)\n",
        "\n",
        "    return tuple(binned_states)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modified State"
      ],
      "metadata": {
        "id": "MIci2OdzAgLS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xyC6n67PO9Zj"
      },
      "outputs": [],
      "source": [
        "# deine modified state after discretizing\n",
        "def modified_state(state):\n",
        "  li = []\n",
        "  li.append((state[0][0]))\n",
        "  li.append((state[0][2] + state[0][1])/2)\n",
        "  li.append((state[0][3]))\n",
        "  li.append((state[0][5] + state[0][4])/2)\n",
        "  li.append((state[0][9]))\n",
        "  li.append((state[0][10]))\n",
        "  li.append((state[0][11]))\n",
        "  return li"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GLhzj37cSPb_"
      },
      "outputs": [],
      "source": [
        "# deine modified state after discretizing\n",
        "def modified_state2(state):\n",
        "  li = []\n",
        "  li.append((state[0]))\n",
        "  li.append((state[2] + state[1])/2)\n",
        "  li.append((state[3]))\n",
        "  li.append((state[5] + state[4])/2)\n",
        "  li.append((state[9]))\n",
        "  li.append((state[10]))\n",
        "  li.append((state[11]))\n",
        "  return li"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# define Q-Table"
      ],
      "metadata": {
        "id": "MiSAunXBAmj5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j45t2_GrAiJN"
      },
      "outputs": [],
      "source": [
        "# define Q-table\n",
        "Q = np.zeros((NUM_BINS,NUM_BINS,NUM_BINS,NUM_BINS,NUM_BINS,NUM_BINS,NUM_BINS,env.action_space.n))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define parameters"
      ],
      "metadata": {
        "id": "aH4pQhN0AqOw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fkh1RMc0APKJ"
      },
      "outputs": [],
      "source": [
        "# define parameters\n",
        "alpha = 0.9\n",
        "GAMMA = 1\n",
        "epsilon = 1\n",
        "train_episodes = 30_000"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q-Learning Equation\n",
        "\n",
        "The Q-learning algorithm updates the action-value function \\( Q(s, a) \\) using the following equation:\n",
        "\n",
        "Q(sₜ, aₜ) ← Q(sₜ, aₜ) + α * [rₜ₊₁ + γ * maxₐ Q(sₜ₊₁, a) - Q(sₜ, aₜ)]\n",
        "\n",
        "\n",
        "Where:\n",
        "- Q(sₜ, aₜ) = current Q-value for state sₜ and action aₜ\n",
        "- α (alpha) = learning rate (0 < α ≤ 1)\n",
        "- γ (gamma) = discount factor (0 ≤ γ < 1)\n",
        "- rₜ₊₁ = immediate reward after action aₜ\n",
        "- maxₐ Q(sₜ₊₁, a) = maximum future value for next state sₜ₊₁\n",
        "- sₜ₊₁ = next state after action aₜ\n",
        "\n",
        "| Component           | Symbol | Description                          |\n",
        "|---------------------|--------|--------------------------------------|\n",
        "| Current Q-value     | Q(s,a) | Value for current state-action pair  |\n",
        "| Learning rate       | α      | How quickly new info overrides old   |\n",
        "| Discount factor     | γ      | Importance of future rewards         |\n",
        "| Immediate reward    | r      | Reward after taking action           |\n",
        "| Max future Q-value  | maxQ   | Best estimated value of next state   |"
      ],
      "metadata": {
        "id": "oeJRw4A4qQ7K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Policy, reduce alpha and reduce epsilon Function\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9DJ-TVACAtHK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zL-8i4A9TaAW"
      },
      "outputs": [],
      "source": [
        "def policy(epsilon, Q, state):\n",
        "    if np.random.random() > epsilon:\n",
        "        action = np.argmax(Q[state])\n",
        "    else:\n",
        "        action = env.action_space.sample()\n",
        "\n",
        "    return action"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def reduce_alpha(episode):\n",
        "    alpha = np.exp(-(episode/10_000))\n",
        "\n",
        "    return alpha"
      ],
      "metadata": {
        "id": "vO7Kx554-Gex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1uLsd80ETgh9"
      },
      "outputs": [],
      "source": [
        "def reduce_epsilon(episode):\n",
        "\n",
        "    epsilon = np.exp(-(episode/1_000))\n",
        "\n",
        "    return epsilon"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # @title Q-Learning Parameter Explorer { run: \"auto\" }\n",
        "\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# from ipywidgets import interact, FloatSlider\n",
        "\n",
        "# # Q-learning parameters with interactive sliders\n",
        "# alpha = FloatSlider(value=0.1, min=0.01, max=1.0, step=0.01, description='Learning Rate (α):')\n",
        "# gamma = FloatSlider(value=0.9, min=0.0, max=0.99, step=0.01, description='Discount (γ):')\n",
        "# epsilon = FloatSlider(value=0.1, min=0.0, max=1.0, step=0.01, description='Exploration (ε):')\n",
        "\n",
        "# def q_learning_update(current_q, reward, max_future_q, alpha, gamma):\n",
        "#     \"\"\"Visualize the Q-learning update equation\"\"\"\n",
        "#     new_q = current_q + alpha * (reward + gamma * max_future_q - current_q)\n",
        "\n",
        "#     # Create visualization\n",
        "#     plt.figure(figsize=(10, 6))\n",
        "#     bars = plt.bar(['Current Q', 'Immediate Reward', 'Max Future Q', 'Updated Q'],\n",
        "#                    [current_q, reward, max_future_q, new_q],\n",
        "#                    color=['blue', 'green', 'orange', 'red'])\n",
        "\n",
        "#     plt.title('Q-Learning Update Visualization', fontsize=14)\n",
        "#     plt.ylabel('Value', fontsize=12)\n",
        "#     plt.xticks(fontsize=10)\n",
        "\n",
        "#     # Add value labels on top of bars\n",
        "#     for bar in bars:\n",
        "#         height = bar.get_height()\n",
        "#         plt.text(bar.get_x() + bar.get_width()/2., height,\n",
        "#                  f'{height:.2f}',\n",
        "#                  ha='center', va='bottom')\n",
        "\n",
        "#     # Display the equation with current values\n",
        "#     equation = f\"Q(s,a) ← {current_q:.2f} + {alpha:.2f} × ({reward:.2f} + {gamma:.2f}×{max_future_q:.2f} - {current_q:.2f}) = {new_q:.2f}\"\n",
        "#     plt.text(0.5, -0.3, equation, ha='center', va='center', transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='yellow', alpha=0.5))\n",
        "\n",
        "#     plt.ylim(0, max(current_q, reward, max_future_q, new_q) * 1.2)\n",
        "#     plt.show()\n",
        "\n",
        "# # Create interactive widget\n",
        "# interact(q_learning_update,\n",
        "#          current_q=FloatSlider(value=5.0, min=0, max=10, step=0.1, description='Current Q:'),\n",
        "#          reward=FloatSlider(value=3.0, min=-5, max=10, step=0.5, description='Reward:'),\n",
        "#          max_future_q=FloatSlider(value=7.0, min=0, max=10, step=0.5, description='Max Future Q:'),\n",
        "#          alpha=alpha,\n",
        "#          gamma=gamma);"
      ],
      "metadata": {
        "id": "mHn4GQK3tjMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q-Learning Algorithm"
      ],
      "metadata": {
        "id": "0cRyFfmQA_z-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PV4SXJHDTjbH"
      },
      "outputs": [],
      "source": [
        "training_rewards = []\n",
        "training_rewards_mean = []\n",
        "epsilons = []\n",
        "log_interval = 1000\n",
        "for episode in tqdm(range(train_episodes)):\n",
        "    state = env.reset()\n",
        "    state = discretize_states(modified_state(state), bins)\n",
        "    total_training_reward = 0\n",
        "    terminated = False\n",
        "    while not terminated:\n",
        "        action = policy(epsilon, Q, state)\n",
        "        new_state,reward,terminated , done, info = env.step(action)\n",
        "        new_state = discretize_states(modified_state2(new_state), bins)\n",
        "        Q[state + (action,)] = Q[state + (action,)] +  alpha * (reward + GAMMA * np.max(Q[new_state]) - Q[state + (action,)])\n",
        "        total_training_reward += reward\n",
        "        state = new_state\n",
        "    if episode % log_interval == 0:\n",
        "        mean_rewards = np.mean(training_rewards[-log_interval:])\n",
        "        log = f\"\"\"\n",
        "            episode {episode}\n",
        "            -----------------------------\n",
        "            Mean Reward = {mean_rewards}\n",
        "            Epsilon = {epsilon}\n",
        "            Alpha = {alpha}\n",
        "            -----------------------------\n",
        "        \"\"\"\n",
        "        print(log)\n",
        "        if episode != 0:\n",
        "            training_rewards_mean.append(mean_rewards)\n",
        "    alpha = reduce_alpha(episode)\n",
        "    epsilon = reduce_epsilon(episode)\n",
        "    training_rewards.append(total_training_reward)\n",
        "    epsilons.append(epsilon)\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plot the Convergence"
      ],
      "metadata": {
        "id": "7f1-V_Y3BDtw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSlCrS1jXcqc"
      },
      "outputs": [],
      "source": [
        "x = range(train_episodes)\n",
        "plt.plot(x, training_rewards)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test Q*-Table"
      ],
      "metadata": {
        "id": "g7etk08IBHMO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8NuZbhbZ9ua"
      },
      "outputs": [],
      "source": [
        "env_v = wrap_env(gymnasium.make(\"FlappyBird-v0\", render_mode=\"rgb_array\", use_lidar=False))\n",
        "state = env_v.reset()\n",
        "state = discretize_states(modified_state(state), bins)\n",
        "done = False\n",
        "c = 0\n",
        "while not done:\n",
        "\n",
        "    # env_v.render(mode='rgb_array')\n",
        "    action = np.argmax(Q[state])\n",
        "    new_state, reward, done, info ,a= env_v.step(action)\n",
        "    state = discretize_states(modified_state2(new_state), bins)\n",
        "\n",
        "\n",
        "env_v.close()\n",
        "show_video()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}